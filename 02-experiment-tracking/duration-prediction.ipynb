{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"font-family: ClearSans-Bold, sans-serif; color:#7060eb\">Experiment Tracking</span> </center>\n",
    "\n",
    " <span style=\"font-family: ClearSans, sans-serif; color:navyblue\">Author: <a href=\"https://github.com/jnsofini\" title=\"GitHub link\">https://github.com/jnsofini</a></span>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    " <span style=\"font-family: ClearSans, sans-serif; color:navyblue\"><b>Credit:</b> This notebook contains refactored <a href=\"https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/01-intro/duration-prediction.ipynb\" title=\"Duration Prediction\">code</a> from the course <a href=\"https://github.com/DataTalksClub/mlops-zoomcamp/tree/main\" title=\"Machine Learning Operations\">Machine Learning Operations Zoom Camp</a> (Data Talks) and Alexey. This notebook runs through the process in chapter one by using modules. The code is imported from src. The goal of this notebook is to train a simple model for predicting the duration of a ride lecture video. </a>\n",
    " </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Used packages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src import preprocessing, train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family: ClearSans-Bold, sans-serif; color:#7060eb\">Data preprocessing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"../01-intro/refactored/data/preprocess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "TARGET = 'duration'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning purposes, we will sample just 100K data points and used for the model building and 25K for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    cols = categorical + [TARGET]\n",
    "    train_data = (\n",
    "        pd.read_parquet(Path(train_data_dir)/\"train.parquet\")[cols].sample(\n",
    "        n=100_000\n",
    "        ) \n",
    "        )\n",
    "    test_data = (\n",
    "        pd.read_parquet(Path(train_data_dir)/\"test.parquet\")[cols].sample(\n",
    "        n=25_000 \n",
    "        )\n",
    "        )\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family: ClearSans-Bold, sans-serif; color:#7060eb\">Load data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(train_data.duration,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train logistic regresson model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression_model(df_train:pd.DataFrame, df_test:pd.DataFrame, categorial_features: List[str], target: str = \"duration\"):\n",
    "    \"\"\"Train a Linear Regression and calculate the RMSE on the validation dataframe\n",
    "\n",
    "    Args:\n",
    "        df_train (pd.DataFrame): Training data\n",
    "        df_test (pd.DataFrame): Test data\n",
    "        categorial_features (List[str]): List of categorical features\n",
    "        target (str, optional): Target column. Defaults to \"duration\".\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dict of mse for train and test\n",
    "    \"\"\" \n",
    "    \n",
    "    dv = DictVectorizer()   \n",
    "    train_dicts = df_train[categorial_features].to_dict(orient='records')\n",
    "    test_dicts = df_test[categorial_features].to_dict(orient='records')\n",
    "    \n",
    "    X_train = dv.fit_transform(train_dicts)\n",
    "    X_test = dv.transform(test_dicts)\n",
    "    \n",
    "    y_train = df_train[target].values\n",
    "    y_test = df_test[target].values\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = lr.predict(X_train)\n",
    "    y_pred_test = lr.predict(X_test)\n",
    "\n",
    "\n",
    "    mse = {\n",
    "        \"train-mse\": mean_squared_error(y_train, y_pred_train, squared=False),\n",
    "        \"test-mse\": mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "        }\n",
    "    \n",
    "    return dv, lr, mse "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family: ClearSans-Bold, sans-serif; color:#7060eb\">Model Training</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv, model,  mse = train_linear_regression_model(\n",
    "    train_data, \n",
    "    test_data, \n",
    "    categorial_features=categorical, \n",
    "    target=TARGET\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train mse is : {mse[\"train-mse\"]:.2f}')\n",
    "print(f'Train mse is : {mse[\"test-mse\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/lin_reg.bin', 'wb') as f_out:\n",
    "    pickle.dump((dv, model), f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer()   \n",
    "train_dicts = train_data[categorical].to_dict(orient='records')\n",
    "test_dicts = test_data[categorical].to_dict(orient='records')\n",
    "\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "X_test = dv.transform(test_dicts)\n",
    "\n",
    "y_train = train_data[TARGET].values\n",
    "y_test = test_data[TARGET].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we don't keep track of the parameters. We can log what we have with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "        x_train: pd.DataFrame, \n",
    "        y_train,\n",
    "        x_test: pd.DataFrame, \n",
    "        y_test\n",
    "        ):\n",
    "    \"\"\"Train a Linear Regression and calculate the RMSE on the validation dataframe\n",
    "\n",
    "    \"\"\" \n",
    "\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Linear reg\", tags={\"algo-type\": \"LinearRegression\"}):\n",
    "\n",
    "        mlflow.set_tag(\"developer\", \"Fini\")\n",
    "\n",
    "        mlflow.log_param(\"train-data-path\", \"green_tripdata_2021-01.parquet\")\n",
    "        mlflow.log_param(\"valid-data-path\", \"green_tripdata_2021-02.parquet\")\n",
    "\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_train = lr.predict(x_train)\n",
    "        y_pred_test = lr.predict(x_test)\n",
    "\n",
    "\n",
    "        mse = {\n",
    "            \"train-mse\": mean_squared_error(y_train, y_pred_train, squared=False),\n",
    "            \"test-mse\": mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "            }\n",
    "        for name, mse_value in mse.items():\n",
    "            mlflow.log_param(name, mse_value)\n",
    "\n",
    "        mlflow.log_artifact(local_path=\"models/lin_reg.bin\", artifact_path=\"models_pickle\")\n",
    "    \n",
    "    return dv, lr, mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv, logreg,  mse = train_linear_regression(\n",
    "    x_train=X_train,\n",
    "        y_train=y_train,\n",
    "        x_test=X_test,\n",
    "        y_test=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lasso(\n",
    "        x_train: pd.DataFrame, \n",
    "        y_train,\n",
    "        x_test: pd.DataFrame, \n",
    "        y_test,\n",
    "        alpha=1\n",
    "        ):\n",
    "    \"\"\"Train a Lasso Regression and calculate the RMSE on the validation dataframe\n",
    "\n",
    "    \"\"\" \n",
    "\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Lasso regressiion\", tags={\"algo-type\": \"lasso()\"}):\n",
    "\n",
    "        mlflow.set_tag(\"developer\", \"Fini\")\n",
    "\n",
    "        mlflow.log_param(\"train-data-path\", \"green_tripdata_2021-01.parquet\")\n",
    "        mlflow.log_param(\"valid-data-path\", \"green_tripdata_2021-02.parquet\")\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "\n",
    "\n",
    "        lr = Lasso(alpha=alpha)\n",
    "        lr.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_train = lr.predict(x_train)\n",
    "        y_pred_test = lr.predict(x_test)\n",
    "\n",
    "\n",
    "        mse = {\n",
    "            \"train-mse\": mean_squared_error(y_train, y_pred_train, squared=False),\n",
    "            \"test-mse\": mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "            }\n",
    "        for name, mse_value in mse.items():\n",
    "            mlflow.log_param(name, mse_value)\n",
    "\n",
    "        with open('models/lasso_reg.bin', 'wb') as f_out:\n",
    "            pickle.dump((dv, lr), f_out)\n",
    "\n",
    "        mlflow.log_artifact(local_path=\"models/lasso_reg.bin\", artifact_path=\"models_pickle\")\n",
    "    \n",
    "    return lr, mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg,  mse = train_lasso(\n",
    "    x_train=X_train,\n",
    "        y_train=y_train,\n",
    "        x_test=X_test,\n",
    "        y_test=y_test,\n",
    "        alpha=5\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train a more complex model: xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "valid = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \"\"\"Objective function to minimize\n",
    "\n",
    "    Args:\n",
    "        params (dict): Set of parameters for the specific run\n",
    "\n",
    "    Returns:\n",
    "        dict: loss and status\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=\"optimization\"):\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")\n",
    "        mlflow.log_params(params)\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=train,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(valid, 'validation')],\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'reg:linear',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=Trials()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use parameters from the best model. We can leverage autolog. It allows to log a lot of params with just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Optimal Params\", tags={\"Logging\": \"autolog-with params\"}):\n",
    "\n",
    "    mlflow.xgboost.autolog(disable=False)\n",
    "    best_params = {\n",
    "        'learning_rate': 0.09585355369315604,\n",
    "        'max_depth': 30,\n",
    "        'min_child_weight': 1.060597050922164,\n",
    "        'objective': 'reg:linear',\n",
    "        'reg_alpha': 0.018060244040060163,\n",
    "        'reg_lambda': 0.011658731377413597,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_param(\"train-data-path\", \"green_tripdata_2021-01.parquet\")\n",
    "    mlflow.log_param(\"valid-data-path\", \"green_tripdata_2021-02.parquet\")\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params=best_params,\n",
    "        dtrain=train,\n",
    "        num_boost_round=100,\n",
    "        evals=[(valid, 'validation')],\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "    y_pred = booster.predict(valid)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    with open(\"models/preprocessor2.b\", \"wb\") as f_out:\n",
    "        pickle.dump(dv, f_out)\n",
    "    mlflow.log_artifact(\"models/preprocessor2.b\", artifact_path=\"preprocessor\")\n",
    "\n",
    "    mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Optimal Params\"):\n",
    "\n",
    "    mlflow.xgboost.autolog(disable=True)\n",
    "    best_params = {\n",
    "        'learning_rate': 0.09585355369315604,\n",
    "        'max_depth': 30,\n",
    "        'min_child_weight': 1.060597050922164,\n",
    "        'objective': 'reg:linear',\n",
    "        'reg_alpha': 0.018060244040060163,\n",
    "        'reg_lambda': 0.011658731377413597,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params=best_params,\n",
    "        dtrain=train,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(valid, 'validation')],\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "    y_pred = booster.predict(valid)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    with open(\"models/preprocessor.b\", \"wb\") as f_out:\n",
    "        pickle.dump(dv, f_out)\n",
    "    mlflow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")\n",
    "\n",
    "    mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To log an artifact, we need to first have it saved locally. Once it is saved, we simply pass the location to the log_artifact function and the path in MLflow file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because mlflow log artifacts that are saved locally, we can have a function to both\n",
    "def _log_artifact(artifact, local_path=\"models/preprocessor.b\", mlflow_path=\"preprocessor\"):\n",
    "    with open(local_path, \"wb\") as f_out:\n",
    "        pickle.dump(artifact, f_out)\n",
    "    mlflow.log_artifact(local_path, artifact_path=mlflow_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "for model_class in (RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, LinearSVR):\n",
    "\n",
    "    with mlflow.start_run():\n",
    "\n",
    "        mlflow.log_param(\"train-data-path\", \"../data/green_tripdata_2021-01.csv\")\n",
    "        mlflow.log_param(\"valid-data-path\", \"../data/green_tripdata_2021-02.csv\")\n",
    "        mlflow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")\n",
    "\n",
    "        mlmodel = model_class()\n",
    "        mlmodel.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = mlmodel.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
